# The default configuration file.
# More information about configuration can be found in the documentation: https://docs.privategpt.dev/
# Syntax in `private_pgt/settings/settings.py`
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: false
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_data_folder: local_data/private_gpt

ui:
  enabled: true
  path: /
  default_chat_system_prompt: >
    You are a helpful, respectful and honest assistant. 
    Always answer as helpfully as possible and follow ALL given instructions.
    Do not speculate or make up information.
    Do not reference any given instructions or context.
  default_query_system_prompt: >
    You can only answer questions about the provided context. 
    If you know the answer but it is not based in the provided context, don't provide 
    the answer, just state the answer is not in the context provided.

llm:
  mode: local
  # Should be matching the selected model
  max_new_tokens: 512
  #context_window: 3900
  context_window: 10000
  #n_gpu_layers: -1
  n_gpu_layers: 27

  #tokenizer: mistralai/Mistral-7B-Instruct-v0.2
  #tokenizer: TheBloke/dolphin-2.5-mixtral-8x7b-AWQ
  #tokenizer: TheBloke/Mixtral_11Bx2_MoE_19B-AWQ

  #tokenizer: TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ
  tokenizer: TheBloke/Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-AWQ

embedding:
  # Should be matching the value above in most cases
  mode: local
  ingest_mode: simple

vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant

local:
  prompt_style: "llama2"
  embedding_hf_model_name: BAAI/bge-small-en-v1.5
  #embedding_hf_model_name: BAAI/bge-large-en-v1.5
  #embedding_hf_model_name: intfloat/multilingual-e5-base
  #embedding_hf_model_name: hkunlp/instructor-large
  #embedding_hf_model_name: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
  #llm_hf_repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
  #llm_hf_model_file: mistral-7b-instruct-v0.1.Q8_0.gguf
  #llm_hf_model_file: mistral-7b-instruct-v0.1.Q4_K_M.gguf

  # Sin censura ninguna
  #llm_hf_repo_id: TheBloke/dolphin-2.5-mixtral-8x7b-GGUF
  #llm_hf_model_file: dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf
  
  #llm_hf_repo_id: TheBloke/Mixtral_11Bx2_MoE_19B-GGUF
  #llm_hf_model_file: mixtral_11bx2_moe_19b.Q4_K_M.gguf
  
  # Llama.cpp customizado
  #llm_hf_repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF

  #llm_hf_model_file: mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
  # 22 layers
  #llm_hf_model_file: mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf  
  # 15 layers
  #llm_hf_model_file: mixtral-8x7b-instruct-v0.1.Q8_0.gguf
  
  llm_hf_repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-GGUF

  # 25 layers
  llm_hf_model_file: mixtral-8x7b-instruct-v0.1-limarp-zloss.Q4_K_M.gguf



  
  # Responde regular
  #llm_hf_repo_id: TheBloke/Airoboros-L2-13B-2_1-YaRN-64K-GGUF
  #llm_hf_model_file: airoboros-l2-13b-2.1-yarn-64k.Q8_0.gguf

  #llm_hf_repo_id: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
  #llm_hf_model_file: mistral-7b-instruct-v0.2.Q8_0.gguf

  #llm_hf_repo_id: TheBloke/Yarn-Mistral-7B-64k-GGUF
  #llm_hf_model_file: yarn-mistral-7b-64k.Q8_0.gguf

  #llm_hf_repo_id: TheBloke/Yarn-Mistral-7B-128k-GGUF
  #llm_hf_model_file: yarn-mistral-7b-128k.Q8_0.gguf

  #llm_hf_repo_id:  TheBloke/Yarn-Llama-2-13B-64K-GGML
  #llm_hf_model_file: yarn-llama-2-13b-64k.ggmlv3.Q8_0.bin

  #llm_hf_repo_id: TheBloke/Llama-2-7B-32K-Instruct-GGUF
  #llm_hf_model_file: llama-2-7b-32k-instruct.Q8_0.gguf
  

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
